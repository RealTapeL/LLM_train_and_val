# åŸºäºè¯­è¨€æ¨¡å‹çš„å­¦ç§‘çŸ¥è¯†å¾®è°ƒå’ŒéªŒè¯
## Fine-tuning and validation of subject knowledge based on llm
### æ­¤ä»“åº“ç”¨äºå¯¹å„ç±»è¯­è¨€æ¨¡å‹çš„å­¦ç§‘çŸ¥è¯†å¾®è°ƒï¼ŒåŒ…å«äº†[æŒ‡ä»¤ç²¾è°ƒæ•°æ®](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/data)ã€[æ¨¡å‹æ•ˆæœè¯„æµ‹](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/scripts)ä»¥åŠ[é¢„è®­ç»ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/scripts)å’Œ[ä½¿ç”¨æœ‰æ ‡æ³¨æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/blob/main/scripts/training/run_sft.sh)ï¼Œæ”¯æŒ[llama.cpp](https://github.com/ggerganov/llama.cpp), [vLLM](https://github.com/vllm-project/vllm)ç­‰ç”Ÿæ€ã€‚

# å¦‚ä½•ä½¿ç”¨ï¼Ÿ
## How to use?ğŸ§

### TRAINING
ä½ å¯ä»¥ä½¿ç”¨[é¢„è®­ç»ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/scripts)å’Œ[ä½¿ç”¨æœ‰æ ‡æ³¨æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/blob/main/scripts/training/run_sft.sh)è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒã€‚
#### é¢„è®­ç»ƒæ­¥éª¤
[é¢„è®­ç»ƒä»£ç ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/blob/main/scripts/training/run_clm_pt_with_peft.py)
è¿›å…¥é¡¹ç›®çš„`scripts/training`ç›®å½•ï¼Œè¿è¡Œ`ash run_pt.sh`è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒï¼Œé»˜è®¤ä½¿ç”¨å•å¡ï¼Œ***ç‰¹åˆ«æ³¨æ„ï¼šè¿è¡Œå‰åº”å…ˆä¿®æ”¹è„šæœ¬å¹¶æŒ‡å®šç›¸å…³å‚æ•°ï¼Œè„šæœ¬ä¸­çš„ç›¸å…³å‚æ•°å€¼ä»…ä¾›è°ƒè¯•å‚è€ƒ***ã€‚`run_pt.sh`å†…å®¹ä¸ºï¼š
```
lr=1e-4
lora_rank=64
lora_alpha=128
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=path/to/llm_model/dir
tokenizer_name_or_path=${pretrained_model}
dataset_dir=path/to/pt/data/dir
data_cache=temp_data_cache_dir
per_device_train_batch_size=1
gradient_accumulation_steps=8
block_size=1024
output_dir=output_dir

torchrun --nnodes 1 --nproc_per_node 1 run_clm_pt_with_peft.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${tokenizer_name_or_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --do_train \
    --low_cpu_mem_usage \
    --seed $RANDOM \
    --bf16 \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --save_steps 200 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --block_size ${block_size} \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --lora_dropout ${lora_dropout} \
    --modules_to_save ${modules_to_save} \
    --torch_dtype bfloat16 \
    --load_in_kbits 16 \
    --ddp_find_unused_parameters False
```
