# åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å­¦ç§‘çŸ¥è¯†å¾®è°ƒå’ŒéªŒè¯
## Fine-tuning and validation of subject knowledge based on llm
### æ­¤ä»“åº“ç”¨äºå¯¹å„ç±»å¤§è¯­è¨€æ¨¡å‹çš„å­¦ç§‘çŸ¥è¯†å¾®è°ƒï¼ŒåŒ…å«äº†[æŒ‡ä»¤ç²¾è°ƒæ•°æ®](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/data)ã€[æ¨¡å‹æ•ˆæœè¯„æµ‹](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/scripts)ä»¥åŠ[é¢„è®­ç»ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/scripts)å’Œ[ä½¿ç”¨æœ‰æ ‡æ³¨æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/blob/main/scripts/training/run_sft.sh)ï¼Œæ”¯æŒ[llama.cpp](https://github.com/ggerganov/llama.cpp)ã€[vLLM](https://github.com/vllm-project/vllm)ç­‰ç”Ÿæ€ã€‚

# å¦‚ä½•ä½¿ç”¨ï¼Ÿ
## How to use?ğŸ§

***ä»¥ä¸‹å‡åœ¨Linuxç³»ç»Ÿæµ‹è¯•ï¼Œwinä¸macosç³»ç»Ÿæƒ…å†µæœªçŸ¥***
## å‡†å¤‡å·¥ä½œ
ç¡®ä¿æ‚¨çš„æœºå™¨æœ‰è¶³å¤Ÿçš„å†…å­˜åŠ è½½å®Œæ•´æ¨¡å‹ä»¥è¿›è¡Œåˆå¹¶æ¨¡å‹æ“ä½œ
å®‰è£…ä¾èµ–åº“ï¼ˆé¡¹ç›®æ ¹ç›®å½•requirements.txtï¼‰
```
pip install -r requirements.txt
```


### TRAINING
ä½ å¯ä»¥ä½¿ç”¨æ— æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹çš„é¢„è®­ç»ƒã€‚

#### é¢„è®­ç»ƒæ­¥éª¤
è¿›å…¥é¡¹ç›®çš„scripts/trainingç›®å½•ï¼Œè¿è¡Œbash run_pt.shè¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒï¼Œé»˜è®¤ä½¿ç”¨å•å¡ï¼Œ***ç‰¹åˆ«æ³¨æ„ï¼šè¿è¡Œå‰åº”å…ˆä¿®æ”¹è„šæœ¬å¹¶æŒ‡å®šç›¸å…³å‚æ•°ï¼Œè„šæœ¬ä¸­çš„ç›¸å…³å‚æ•°å€¼ä»…ä¾›è°ƒè¯•å‚è€ƒ***ã€‚run_pt.shå†…å®¹ä¸ºï¼š
```
lr=1e-4
lora_rank=64
lora_alpha=128
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=path/to/llm_model/dir
tokenizer_name_or_path=${pretrained_model}
dataset_dir=path/to/pt/data/dir
data_cache=temp_data_cache_dir
per_device_train_batch_size=1
gradient_accumulation_steps=8
block_size=1024
output_dir=output_dir

torchrun --nnodes 1 --nproc_per_node 1 run_clm_pt_with_peft.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${tokenizer_name_or_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --do_train \
    --low_cpu_mem_usage \
    --seed $RANDOM \
    --bf16 \
    --num_train_epochs 1 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --save_steps 200 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --block_size ${block_size} \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --lora_dropout ${lora_dropout} \
    --modules_to_save ${modules_to_save} \
    --torch_dtype bfloat16 \
    --load_in_kbits 16 \
    --ddp_find_unused_parameters False
```

ä½ è¿˜å¯ä»¥[ä½¿ç”¨æœ‰æ ‡æ³¨æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/blob/main/scripts/training/run_sft.sh)
#### æŒ‡ä»¤å¾®è°ƒæ­¥éª¤
è¿›å…¥é¡¹ç›®çš„scripts/trainingç›®å½•ï¼Œè¿è¡Œbash run_sft.shè¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒï¼Œé»˜è®¤ä½¿ç”¨å•å¡ï¼Œ***ç‰¹åˆ«æ³¨æ„ï¼šè¿è¡Œå‰åº”å…ˆä¿®æ”¹è„šæœ¬å¹¶æŒ‡å®šç›¸å…³å‚æ•°ï¼Œè„šæœ¬ä¸­çš„ç›¸å…³å‚æ•°å€¼ä»…ä¾›è°ƒè¯•å‚è€ƒ***ã€‚run_sft.shå†…å®¹ä¸ºï¼š
```
lr=1e-4
lora_rank=64
lora_alpha=128
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=path/to/hf/llama-3-chinese/dir/or/model_id
dataset_dir=path/to/sft/data/dir
per_device_train_batch_size=1
per_device_eval_batch_size=1
gradient_accumulation_steps=8
max_seq_length=1024
output_dir=output_dir
validation_file=validation_file_name

torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${pretrained_model} \
    --dataset_dir ${dataset_dir} \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --do_eval \
    --seed $RANDOM \
    --bf16 \
    --num_train_epochs 3 \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.1 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --evaluation_strategy steps \
    --eval_steps 100 \
    --save_steps 200 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --max_seq_length ${max_seq_length} \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --lora_dropout ${lora_dropout} \
    --modules_to_save ${modules_to_save} \
    --torch_dtype bfloat16 \
    --validation_file ${validation_file} \
    --load_in_kbits 16 \
```

### æ¨¡å‹åˆå¹¶
è¿™ä¸€æ­¥éª¤ä¼šåˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ï¼ˆsafetensorsæ ¼å¼ï¼‰ã€‚
```

$ python scripts/merge_llama3_with_chinese_lora_low_mem.py
--base_model path_to_original_llama3_dir
--lora_model path_to_lora
--output_dir path_to_output_dir
```
## æ¨¡å‹æ•ˆæœè¯„æµ‹

### æ¨¡å‹ç”Ÿæˆæ•ˆæœè¯„æµ‹
ä¸ºäº†å…¨é¢è¯„ä¼°ç›¸å…³æ¨¡å‹çš„æ€§èƒ½ï¼Œå»ºè®®ç”¨æˆ·æ ¹æ®è‡ªèº«å…³æ³¨çš„ä»»åŠ¡ç‰¹æ€§ï¼Œè¿›è¡Œé’ˆå¯¹æ€§çš„æ¨¡å‹æµ‹è¯•ï¼Œä»¥ç­›é€‰å‡ºæœ€é€‚åˆç‰¹å®šä»»åŠ¡éœ€æ±‚çš„æ¨¡å‹ã€‚[Fastchat Chatbot Arena](https://lmarena.ai/?arena)ï¼Œæ¨å‡ºäº†æ¨¡å‹åœ¨çº¿å¯¹æˆ˜å¹³å°ï¼Œå¯æµè§ˆå’Œè¯„æµ‹æ¨¡å‹å›å¤è´¨é‡ã€‚å¯¹æˆ˜å¹³å°æä¾›äº†èƒœç‡ã€Eloè¯„åˆ†ç­‰è¯„æµ‹æŒ‡æ ‡ï¼Œå¹¶ä¸”å¯ä»¥æŸ¥çœ‹ä¸¤ä¸¤æ¨¡å‹çš„å¯¹æˆ˜èƒœç‡ç­‰ç»“æœã€‚

## æ¨¡å‹å®¢è§‚æ•ˆæœè¯„æµ‹
### C-Eval
[C-Eval](https://github.com/hkust-nlp/ceval) æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ã€‚å®ƒç”± 13948 é“å¤šé¡¹é€‰æ‹©é¢˜ç»„æˆï¼Œæ¶µç›– 52 ä¸ªä¸åŒçš„å­¦ç§‘å’Œ 4 ä¸ªéš¾åº¦çº§åˆ«
åœ¨æŠ±æŠ±è„¸Hugging Faceä¸Šä¸‹è½½è¯„æµ‹æ•°æ®é›†ï¼Œå¹¶è§£å‹
```
wget https://huggingface.co/datasets/ceval/ceval-exam/resolve/main/ceval-exam.zip
unzip ceval-exam.zip -d data
```
### è¿è¡Œé¢„æµ‹è„šæœ¬
```
cd scripts/ceval
python eval.py \
    --model_path ${model_path} \
    --few_shot False \
    --with_prompt False\
    --n_times 1 \
    --ntrain 5 \
    --do_save_csv False \
    --do_test False \
    --output_dir ${output_path} \
```

### æŒ‡ä»¤æ•°æ®
ä»¥ä¸‹æ˜¯æœ¬é¡¹ç›®çš„éƒ¨åˆ†æŒ‡ä»¤æ•°æ®

 æŒ‡ä»¤æ•°æ®åç§°  | è¯´æ˜  
 ----                                                                                                                              | ----- |
[stem_zh_instruction](https://github.com/RealTapeL/Fine-tuning-and-validation-based-on-llama3/tree/main/data/stem_zh_instruction)  | ä½¿ç”¨gpt-3.5çˆ¬å–çš„STEMæ•°æ®ï¼ŒåŒ…å«ç‰©ç†ã€åŒ–å­¦ã€åŒ»å­¦ã€ç”Ÿç‰©å­¦ã€åœ°çƒç§‘å­¦ 
[DPO-zh-en-emoji ](https://huggingface.co/datasets/shareAI/DPO-zh-en-emoji)                                                        | ShareAIæ˜¯å›½å†…çš„ä¼˜ç§€å¼€æºç»„ç»‡ï¼Œä»–ä»¬æ•´ç†çš„ä¸­æ–‡æ•°æ®é›†å¯ä»¥å¯¹å·²æœ‰çš„å¯¹è¯æ¨¡å‹è¿›è¡Œè¯­è¨€è½½ä½“å’Œè¯­è¨€é£æ ¼(å¹½é»˜é£æ ¼ + emoji è¡¨æƒ…) çš„åå¥½å¼ºåŒ–å¯¹é½

## åŸºäº[llama.cpp](https://github.com/ggerganov/llama.cpp)çš„éƒ¨ç½²

è¿è¡Œå‰è¯·ç¡®ä¿ç¯å¢ƒï¼š

ç³»ç»Ÿåº”æœ‰makeï¼ˆMacOS/Linuxè‡ªå¸¦ï¼‰æˆ–cmakeï¼ˆWindowséœ€è‡ªè¡Œå®‰è£…ï¼‰ç¼–è¯‘å·¥å…·
å»ºè®®ä½¿ç”¨Python 3.10ä»¥ä¸Šç¼–è¯‘å’Œè¿è¡Œè¯¥å·¥å…·

Step 1: ç¼–è¯‘llama.cpp

llama.cppåœ¨2024å¹´4æœˆ30æ—¥å¯¹ç¼–è¯‘åšå‡ºé‡å¤§æ”¹åŠ¨ï¼Œè¯·åŠ¡å¿…æ‹‰å–æœ€æ–°ä»“åº“è¿›è¡Œç¼–è¯‘ï¼
```
$ git clone https://github.com/ggerganov/llama.cpp
```
å¯¹llama.cppé¡¹ç›®è¿›è¡Œç¼–è¯‘ï¼Œç”Ÿæˆ./mainï¼ˆç”¨äºæ¨ç†ï¼‰å’Œ./quantizeï¼ˆç”¨äºé‡åŒ–ï¼‰äºŒè¿›åˆ¶æ–‡ä»¶
```
$ make
```
Windows/Linuxç”¨æˆ·å¦‚éœ€å¯ç”¨GPUæ¨ç†ï¼Œåˆ™æ¨èä½¿ç”¨cudaç¼–è¯‘
```
$ make LLAMA_CUDA=1
```

Step 2:ç”Ÿæˆé‡åŒ–ç‰ˆæœ¬(ggufæ ¼å¼)æ¨¡å‹

ç›®å‰llama.cppå·²æ”¯æŒ.safetensorsæ–‡ä»¶ä»¥åŠHugging Faceæ ¼å¼.binè½¬æ¢ä¸ºFP16çš„GGUFæ ¼å¼
```
$ python convert-hf-to-gguf.py your_model_name
$ ./quantize your_model_name/ggml-model-f16.gguf your_model_name/ggml-model-q4_0.gguf q4_0
```

Step 3: åŠ è½½å¹¶å¯åŠ¨æ¨¡å‹
## Linuxã€macOSç­‰çš„ç³»ç»Ÿï¼š
#### å¯¹è¯æ¨¡å¼ï¼ˆå…è®¸ä¸æ¨¡å‹æŒç»­äº¤äº’ï¼‰
```
./llama-cli -m /home/fw/results8.24/earthllm826/earth.gguf -p "You are a helpful assistant" -cnv --chat-template chatml
```
## Windowsç³»ç»Ÿï¼š
#### å¯¹è¯æ¨¡å¼ï¼ˆå…è®¸ä¸æ¨¡å‹æŒç»­äº¤äº’ï¼‰
```
./llama-cli.exe -m /home/fw/results8.24/earthllm826/earth.gguf -p "You are a helpful assistant" -cnv --chat-template chatml
```
